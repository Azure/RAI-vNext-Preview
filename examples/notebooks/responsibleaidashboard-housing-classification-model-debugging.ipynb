{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587f4596",
   "metadata": {},
   "source": [
    "# Debug housing price predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6550b0",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of the AzureML RAI components to assess a classification model trained on Kaggle's apartments dataset (https://www.kaggle.com/alphaepsilon/housing-prices-dataset). The model predicts if the house sells for more than median price or not. It is a reimplementation of the [notebook of the same name](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-classification-model-debugging.ipynb) in the [Responsible AI toolbox repo](https://github.com/microsoft/responsible-ai-toolbox).\n",
    "\n",
    "First, we need to specify the version of the RAI components which are available in the workspace. This was specified when the components were uploaded, and will have defaulted to '1':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f41ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f81e5d",
   "metadata": {},
   "source": [
    "We also need to give the name of the compute cluster we want to use in AzureML. Later in this notebook, we will create it if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = \"cpucluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f25c3",
   "metadata": {},
   "source": [
    "## Accessing the Data\n",
    "\n",
    "The following section examines the code necessary to create datasets and a model using components in AzureML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736295c",
   "metadata": {},
   "source": [
    "### Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927997ce",
   "metadata": {},
   "source": [
    "First, we load the data from the blob store, do some basic data cleaning, and split in to training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "target_feature = 'Sold_HigherThan_Median'\n",
    "categorical_features = []\n",
    "\n",
    "outdirname = 'responsibleai.12.28.21'\n",
    "try:\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.request import urlretrieve\n",
    "zipfilename = outdirname + '.zip'\n",
    "urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, zipfilename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as unzip:\n",
    "    unzip.extractall('.')\n",
    "\n",
    "all_data = pd.read_csv('apartments-train.csv')\n",
    "all_data = all_data.drop(['SalePrice','SalePriceK'], axis=1)\n",
    "X, y = split_label(all_data, target_feature)\n",
    "\n",
    "\n",
    "X_train_original, X_test_original, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=7, stratify=y)\n",
    "\n",
    "train_data = X_train_original.copy()\n",
    "train_data[target_feature] = y_train\n",
    "\n",
    "test_data = X_test_original.copy()\n",
    "test_data[target_feature] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4305b27",
   "metadata": {},
   "source": [
    "### Get the Data to AzureML\n",
    "\n",
    "With the data now split into 'train' and 'test' DataFrames, we save them out to files in preparation for upload into AzureML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving to files\")\n",
    "train_data.to_parquet(\"housing_train.parquet\", index=False)\n",
    "test_data.to_parquet(\"housing_test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc435",
   "metadata": {},
   "source": [
    "We are going to create two Datasets in AzureML, one for the train and one for the test datasets. The first step is to create an `MLClient` to perform the upload. The method we use assumes that there is a `config.json` file (downloadable from the Azure or AzureML portals) present in the same directory as this notebook file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a4e2b",
   "metadata": {},
   "source": [
    "We can then define the Datasets, and create them in AzureML. This will also upload the Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    name=\"Housing_Train_from_Notebook\",\n",
    "    local_path=\"housing_train.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(train_dataset)\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    name=\"Housing_Test_from_Notebook\",\n",
    "    local_path=\"housing_test.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1edb8",
   "metadata": {},
   "source": [
    "## A model training pipeline\n",
    "\n",
    "To simplify the model creation process, we're going to use a pipeline. This will have two stages:\n",
    "\n",
    "1. The actual training component\n",
    "1. A model registration component\n",
    "\n",
    "We have to register the model in AzureML in order for our RAI insights components to use it.\n",
    "\n",
    "### The Training Component\n",
    "\n",
    "The training component is for this particular model. First, we write the training script which will be executed. In this case, we are going to train an `LCBMClassifier` on the input data and save it using MLFlow. We need command line arguments to specify the location of the input data, the location where MLFlow should write the output model, and the name of the target column (i.e. `y`) in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e07198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile housing_training_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = LGBMClassifier(n_estimators=5)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad789b",
   "metadata": {},
   "source": [
    "Now that the script is saved on our local drive, we can use the AzureML SDKv2 to describe the component, and our `MLClient` object to register it with AzureML. First, we choose a version for this component (which need not match the version of the RAI components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67490e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_component_version_string = '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd9efb",
   "metadata": {},
   "source": [
    "Now, define and create the component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c312d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Code, CommandComponent\n",
    "\n",
    "training_code = Code(\n",
    "    local_path='housing_training_script.py'\n",
    ")\n",
    "\n",
    "training_inputs = {\n",
    "    'training_data': { 'type': 'path'},\n",
    "    'target_column_name': { 'type': 'string'}\n",
    "}\n",
    "\n",
    "training_outputs = {\n",
    "    'model_output': { 'type': 'path'}\n",
    "}\n",
    "\n",
    "training_component = CommandComponent(\n",
    "    name=\"HousingTrainingComponent\",\n",
    "    version=training_component_version_string,\n",
    "    display_name=\"Simple training component for housing Dataset\",\n",
    "    code=training_code,\n",
    "    environment=f\"AML-RAI-Environment:{version_string}\",\n",
    "    inputs=training_inputs,\n",
    "    outputs=training_outputs,\n",
    "    command=\"python housing_training_script.py \" \\\n",
    "            \"--training_data ${{inputs.training_data}} \" \\\n",
    "            \"--target_column_name ${{inputs.target_column_name}} \" \\\n",
    "            \"--model_output ${{outputs.model_output}}\"\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(training_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d994bd",
   "metadata": {},
   "source": [
    "We need a compute target on which to run our jobs. The following checks whether the compute specified above is present; if not, then the compute target is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_DS2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ec415",
   "metadata": {},
   "source": [
    "### Running a training pipeline\n",
    "\n",
    "The component to register the model is part of the suite of RAI components, so we do not have to define it here. As such, we are now ready to run the training pipeline itself.\n",
    "\n",
    "We start by defining the name under which we want to register the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ml.entities import JobInput, ComponentJob, PipelineJob\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'my_housing_nb_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f3361",
   "metadata": {},
   "source": [
    "Next, we define the pipeline using objects from the AzureML SDKv2. As mentioned above, there are two component jobs: one to train the model, and one to register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449dcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall inputs for the pipeline\n",
    "\n",
    "pipeline_inputs = {\n",
    "    'target_column_name': target_feature,\n",
    "    'my_training_data': JobInput(dataset=f\"Housing_Train_from_Notebook:1\", mode=\"download\"),\n",
    "    'my_test_data': JobInput(dataset=f\"Housing_Test_from_Notebook:1\", mode=\"download\")\n",
    "}\n",
    "\n",
    "# Specify the training job\n",
    "train_job_inputs = {\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'training_data': '${{inputs.my_training_data}}',\n",
    "}\n",
    "train_job_outputs = {\n",
    "    'model_output': None\n",
    "}\n",
    "train_job = ComponentJob(\n",
    "    component=f\"HousingTrainingComponent:{training_component_version_string}\",\n",
    "    inputs=train_job_inputs,\n",
    "    outputs=train_job_outputs\n",
    ")\n",
    "\n",
    "# The model registration job\n",
    "register_job_inputs = {\n",
    "    'model_input_path': '${{jobs.train-model-job.outputs.model_output}}',\n",
    "    'model_base_name': model_name,\n",
    "    'model_name_suffix': model_name_suffix\n",
    "}\n",
    "register_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "register_job = ComponentJob(\n",
    "    component=f\"register_model:{version_string}\",\n",
    "    inputs=register_job_inputs,\n",
    "    outputs=register_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca016b",
   "metadata": {},
   "source": [
    "With our jobs specified, assemble them into a pipeline. You can substitute the name of your own compute in place of `cpucluster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121282c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registration_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Register_Housing_Model_From_Notebook_01\",\n",
    "    description=\"Create and register a model from a notebook\",\n",
    "    jobs={\n",
    "        'train-model-job': train_job,\n",
    "        'register-model-job': register_job,\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=register_job_outputs,\n",
    "    compute=compute_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397292c",
   "metadata": {},
   "source": [
    "And submit it to AzureML. We define a helper function to do the submission, which waits for the submitted job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job\n",
    "\n",
    "# This is the actual submission\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29ec13",
   "metadata": {},
   "source": [
    "##  Creating the RAI Insights\n",
    "\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147d43f",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are four 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty `RAIInsights` object\n",
    "1. Run the RAI tool components\n",
    "1. Gather the tool outputs into a single `RAIInsights` object\n",
    "\n",
    "The job to fetch the registered model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9357f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be necessary once models are types within the pipeline graph\n",
    "\n",
    "fetch_job_inputs = {\n",
    "    'model_id': expected_model_id\n",
    "}\n",
    "fetch_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "fetch_job = ComponentJob(\n",
    "    component=f\"fetch_registered_model:{version_string}\",\n",
    "    inputs=fetch_job_inputs,\n",
    "    outputs=fetch_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedaafbd",
   "metadata": {},
   "source": [
    "With this registered model (and our datasets), we can create an empty RAI dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Top level RAI Insights component\n",
    "\n",
    "# We will reuse the same pipeline_inputs object in the end\n",
    "create_rai_inputs = {\n",
    "    'title': 'Run built from a Notebook',\n",
    "    'task_type': 'classification',\n",
    "    'model_info_path': '${{jobs.fetch-model-job.outputs.model_info_output_path}}',\n",
    "    'train_dataset': '${{inputs.my_training_data}}',\n",
    "    'test_dataset': '${{inputs.my_test_data}}',\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'categorical_column_names': json.dumps(categorical_features),\n",
    "    'classes': '[\"Less than median\", \"More than median\"]'\n",
    "}\n",
    "create_rai_outputs = {\n",
    "    'rai_insights_dashboard': None # Could theoretically redirect the datastore here\n",
    "}\n",
    "create_rai_job = ComponentJob(\n",
    "    component=f\"rai_insights_constructor:{version_string}\",\n",
    "    inputs=create_rai_inputs,\n",
    "    outputs=create_rai_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61ea84",
   "metadata": {},
   "source": [
    "Now, create instances of our RAI tools. Each of the tools has its own component, which accepts the same arguments as the corresponding manager of the `RAIInsights` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the explanation\n",
    "explain_inputs = {\n",
    "   'comment': 'Insert text here',\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}'\n",
    "}\n",
    "explain_outputs = {\n",
    "    'explanation': None\n",
    "}\n",
    "explain_job = ComponentJob(\n",
    "    component=f\"rai_insights_explanation:{version_string}\",\n",
    "    inputs=explain_inputs,\n",
    "    outputs=explain_outputs\n",
    ")\n",
    "\n",
    "\n",
    "# Setup counterfactual\n",
    "counterfactual_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'total_CFs': '10',\n",
    "    'desired_class': 'opposite'\n",
    "}\n",
    "counterfactual_outputs = {\n",
    "    'counterfactual': None\n",
    "}\n",
    "counterfactual_job = ComponentJob(\n",
    "    component=f\"rai_insights_counterfactual:{version_string}\",\n",
    "    inputs=counterfactual_inputs,\n",
    "    outputs=counterfactual_outputs\n",
    ")\n",
    "\n",
    "# Setup error analysis\n",
    "error_analysis_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "}\n",
    "error_analysis_outputs = {\n",
    "    'error_analysis': None\n",
    "}\n",
    "error_analysis_job = ComponentJob(\n",
    "    component=f\"rai_insights_erroranalysis:{version_string}\",\n",
    "    inputs=error_analysis_inputs,\n",
    "    outputs=error_analysis_outputs\n",
    ")\n",
    "\n",
    "# Setup causal\n",
    "causal_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'treatment_features': '[\"OverallCond\", \"OverallQual\", \"Fireplaces\", \"GarageCars\", \"ScreenPorch\"]',\n",
    "}\n",
    "causal_outputs = {\n",
    "    'causal': None\n",
    "}\n",
    "causal_job = ComponentJob(\n",
    "    component=f\"rai_insights_causal:{version_string}\",\n",
    "    inputs=causal_inputs,\n",
    "    outputs=causal_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f123ef",
   "metadata": {},
   "source": [
    "Now the 'gather' component which assembles everything into an `RAIInsights` object, and computes the JSON for the UX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf611f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the gather component\n",
    "gather_inputs = {\n",
    "    'constructor': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'insight_1': '${{jobs.explain-job.outputs.explanation}}',\n",
    "    'insight_2': '${{jobs.counterfactual-job.outputs.counterfactual}}',\n",
    "    'insight_3': '${{jobs.error-analysis-job.outputs.error_analysis}}',\n",
    "    'insight_4': '${{jobs.causal-job.outputs.causal}}'\n",
    "}\n",
    "gather_outputs = {\n",
    "    'dashboard': None,\n",
    "    'ux_json': None\n",
    "}\n",
    "gather_job = ComponentJob(\n",
    "    component=f\"rai_insights_gather:{version_string}\",\n",
    "    inputs=gather_inputs,\n",
    "    outputs=gather_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99088d",
   "metadata": {},
   "source": [
    "With all of our jobs defined, we can assemble them into the pipeline itself. Again, the appropriate name for your compute resource should be substituted for `cpucluster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Compute_Housing_Insights_from_Notebook_{version_string}\",\n",
    "    description=\"Python submitted Housing insights using fetched model\",\n",
    "    jobs={\n",
    "        'fetch-model-job': fetch_job,\n",
    "        'create-rai-job': create_rai_job,\n",
    "        'counterfactual-job': counterfactual_job,\n",
    "        'error-analysis-job': error_analysis_job,\n",
    "        'explain-job': explain_job,\n",
    "        'causal-job': causal_job,\n",
    "        'housing-gather-job': gather_job\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=None,\n",
    "    compute=compute_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdb183",
   "metadata": {},
   "source": [
    "Now, submit the pipeline job and wait for it to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d13bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba936b36",
   "metadata": {},
   "source": [
    "Once this is complete, we can go to the Reigstered Models view in the AzureML portal, and find the model we have just registered. On the 'Model Details' page, there is a \"Responsible AI dashboard\" tab where we can view the insights which we have just uploaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

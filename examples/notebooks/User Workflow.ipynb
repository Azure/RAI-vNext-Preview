{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121a2d97",
   "metadata": {},
   "source": [
    "First we fetch the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2ac153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched\n",
      "Saving to files\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = shap.datasets.adult()\n",
    "print(\"Data fetched\")\n",
    "target_feature = \"income\"\n",
    "y = [1 if y_i else 0 for y_i in y]\n",
    "\n",
    "full_data = X.copy()\n",
    "full_data[target_feature] = y\n",
    "\n",
    "data_train, data_test = train_test_split(\n",
    "    full_data, test_size=1000, random_state=96132, stratify=full_data[target_feature]\n",
    ")\n",
    "\n",
    "# Don't write out the row indices to the CSV.....\n",
    "print(\"Saving to files\")\n",
    "data_train.to_parquet(\"adult_train.parquet\", index=False)\n",
    "data_test.to_parquet(\"adult_test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3192b",
   "metadata": {},
   "source": [
    "Now create an MLClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30a0f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\RAI-vNext-Preview\\config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b06c44",
   "metadata": {},
   "source": [
    "Upload the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a309cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading adult_train.parquet\u001b[32m (< 1 MB): 100%|######################################################################################################################################################################| 173k/173k [00:00<00:00, 790kB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({'paths': [<azure.ml._restclient.v2021_10_01.models._models_py3.UriReference object at 0x000001F6C019DD90>], 'is_anonymous': False, 'auto_increment_version': False, 'name': 'Adult_Train_from_Notebook', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/a75ae43f-9f72-4699-ba66-d3a173cfe082/resourceGroups/ilmatrg/providers/Microsoft.MachineLearningServices/workspaces/ilmatworkspace/datasets/Adult_Train_from_Notebook/versions/1', 'base_path': './', 'creation_context': <azure.ml._restclient.v2021_10_01.models._models_py3.SystemData object at 0x000001F6C019DF40>, 'serialize': <msrest.serialization.Serializer object at 0x000001F6C01A0250>, 'version': '1', 'local_path': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    name=\"Adult_Train_from_Notebook\",\n",
    "    local_path=\"adult_train.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30747ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading adult_test.parquet\u001b[32m (< 1 MB): 100%|#####################################################################################################################################################################| 14.8k/14.8k [00:00<00:00, 271kB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({'paths': [<azure.ml._restclient.v2021_10_01.models._models_py3.UriReference object at 0x000001F6C017B0D0>], 'is_anonymous': False, 'auto_increment_version': False, 'name': 'Adult_Test_from_Notebook', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/a75ae43f-9f72-4699-ba66-d3a173cfe082/resourceGroups/ilmatrg/providers/Microsoft.MachineLearningServices/workspaces/ilmatworkspace/datasets/Adult_Test_from_Notebook/versions/1', 'base_path': './', 'creation_context': <azure.ml._restclient.v2021_10_01.models._models_py3.SystemData object at 0x000001F6C017B8E0>, 'serialize': <msrest.serialization.Serializer object at 0x000001F6C017BEE0>, 'version': '1', 'local_path': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset(\n",
    "    name=\"Adult_Test_from_Notebook\",\n",
    "    local_path=\"adult_test.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f72fb",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "To simplify the model creation process, we're going to use a pipeline.\n",
    "\n",
    "Before we do anything else, we need to specify the version of the RAI components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0a123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55f3e6",
   "metadata": {},
   "source": [
    "Now we can create the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd44c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704878b",
   "metadata": {},
   "source": [
    "Now, we want to place this into a component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209c2424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading training_script.py\u001b[32m (< 1 MB): 100%|####################################################################################################################################################################| 2.49k/2.49k [00:00<00:00, 55.9kB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommandComponent({'auto_increment_version': False, 'is_anonymous': False, 'name': 'MyTrainingComponent', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/a75ae43f-9f72-4699-ba66-d3a173cfe082/resourceGroups/ilmatrg/providers/Microsoft.MachineLearningServices/workspaces/ilmatworkspace/components/MyTrainingComponent/versions/2', 'base_path': None, 'creation_context': <azure.ml._restclient.v2021_10_01.models._models_py3.SystemData object at 0x000001F6C01581C0>, 'serialize': <msrest.serialization.Serializer object at 0x000001F6BF918130>, 'command': 'python training_script.py --training_data ${{inputs.training_data}} --target_column_name ${{inputs.target_column_name}} --model_output ${{outputs.model_output}}', 'code': '/subscriptions/a75ae43f-9f72-4699-ba66-d3a173cfe082/resourceGroups/ilmatrg/providers/Microsoft.MachineLearningServices/workspaces/ilmatworkspace/codes/22ca1630-9e43-4d70-9167-3e56c06d3f15/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/a75ae43f-9f72-4699-ba66-d3a173cfe082/resourceGroups/ilmatrg/providers/Microsoft.MachineLearningServices/workspaces/ilmatworkspace/environments/AML-RAI-Environment/versions/1', 'distribution': None, 'resources': OrderedDict([('instance_count', 1)]), 'version': '2', 'schema': 'https://azuremlschemas.azureedge.net/stable/commandComponent.schema.json', 'type': 'command', 'display_name': 'Simple training component', 'is_deterministic': True, 'inputs': {'training_data': {'name': 'training_data', 'optional': 'False', 'type': 'path'}, 'target_column_name': {'name': 'target_column_name', 'optional': 'False', 'type': 'string'}}, 'outputs': {'model_output': {'name': 'model_output', 'type': 'path'}}, 'yaml_str': None, 'other_parameter': {}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml.entities import Code, CommandComponent\n",
    "\n",
    "training_code = Code(\n",
    "    local_path='training_script.py'\n",
    ")\n",
    "\n",
    "training_inputs = {\n",
    "    'training_data': { 'type': 'path'},\n",
    "    'target_column_name': { 'type': 'string'}\n",
    "}\n",
    "\n",
    "training_outputs = {\n",
    "    'model_output': { 'type': 'path'}\n",
    "}\n",
    "\n",
    "training_component = CommandComponent(\n",
    "    name=\"MyTrainingComponent\",\n",
    "    version=\"2\",\n",
    "    display_name=\"Simple training component\",\n",
    "    code=training_code,\n",
    "    environment=f\"AML-RAI-Environment:{version_string}\",\n",
    "    inputs=training_inputs,\n",
    "    outputs=training_outputs,\n",
    "    command=\"python training_script.py \" \\\n",
    "            \"--training_data ${{inputs.training_data}} \" \\\n",
    "            \"--target_column_name ${{inputs.target_column_name}} \" \\\n",
    "            \"--model_output ${{outputs.model_output}}\"\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(training_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cf52f",
   "metadata": {},
   "source": [
    "# Running a training pipeline\n",
    "Now we have a script which can train a model, we need to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf80c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ml.entities import JobInput, ComponentJob, PipelineJob\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'my_trained_nb_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6ff94",
   "metadata": {},
   "source": [
    "This is going to be a two component pipeline. The first will be the one we created above, which will train our model. The second will register it in AzureML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f93027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall inputs for the pipeline\n",
    "\n",
    "pipeline_inputs = {\n",
    "    'target_column_name': 'income',\n",
    "    'my_training_data': JobInput(dataset=f\"Adult_Train_from_Notebook:1\"),\n",
    "    'my_test_data': JobInput(dataset=f\"Adult_Test_from_Notebook:1\")\n",
    "}\n",
    "\n",
    "# Specify the training job\n",
    "train_job_inputs = {\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'training_data': '${{inputs.my_training_data}}',\n",
    "}\n",
    "train_job_outputs = {\n",
    "    'model_output': None\n",
    "}\n",
    "train_job = ComponentJob(\n",
    "    component=f\"MyTrainingComponent:2\",\n",
    "    inputs=train_job_inputs,\n",
    "    outputs=train_job_outputs\n",
    ")\n",
    "\n",
    "# The model registration job\n",
    "register_job_inputs = {\n",
    "    'model_input_path': '${{jobs.train-model-job.outputs.model_output}}',\n",
    "    'model_base_name': model_name,\n",
    "    'model_name_suffix': model_name_suffix\n",
    "}\n",
    "register_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "register_job = ComponentJob(\n",
    "    component=f\"RegisterModel:{version_string}\",\n",
    "    inputs=register_job_inputs,\n",
    "    outputs=register_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9970f",
   "metadata": {},
   "source": [
    "With our jobs specified, assemble them into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e2410db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registration_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Register_Model_From_Notebook_01\",\n",
    "    description=\"Create and register a model from a notebook\",\n",
    "    jobs={\n",
    "        'train-model-job': train_job,\n",
    "        'register-model-job': register_job,\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=register_job_outputs,\n",
    "    compute=\"cpucluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a6422",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d450922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42543ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute is not a known attribute of class <class 'azure.ml._restclient.v2021_10_01.models._models_py3.PipelineJob'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Completed\n"
     ]
    }
   ],
   "source": [
    "# This is the actual submission\n",
    "\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f9e4",
   "metadata": {},
   "source": [
    "# Creating the RAI Insights\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered (this is not straightforward since the Register Model component is used in testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b893bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8c009",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are three 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty RAI dashboard\n",
    "1. Run the RAI tool components\n",
    "\n",
    "The job to fetch the registered model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be necessary once models are types within the pipeline graph\n",
    "\n",
    "fetch_job_inputs = {\n",
    "    'model_id': expected_model_id\n",
    "}\n",
    "fetch_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "fetch_job = ComponentJob(\n",
    "    component=f\"FetchRegisteredModel:{version_string}\",\n",
    "    inputs=fetch_job_inputs,\n",
    "    outputs=fetch_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b192f",
   "metadata": {},
   "source": [
    "With this registered model (and our datasets), we can create an empty RAI dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1440506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level RAI Insights component\n",
    "\n",
    "# We will reuse the same pipeline_inputs object in the end\n",
    "create_rai_inputs = {\n",
    "    'title': 'Run built from a Notebook',\n",
    "    'task_type': 'classification',\n",
    "    'model_info_path': '${{jobs.fetch-model-job.outputs.model_info_output_path}}',\n",
    "    'train_dataset': '${{inputs.my_training_data}}',\n",
    "    'test_dataset': '${{inputs.my_test_data}}',\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'categorical_column_names': '[\"Race\", \"Sex\", \"Workclass\", \"Marital Status\", \"Country\", \"Occupation\"]',\n",
    "}\n",
    "create_rai_outputs = {\n",
    "    'rai_insights_dashboard': None # Could theoretically redirect the datastore here\n",
    "}\n",
    "create_rai_job = ComponentJob(\n",
    "    component=f\"RAIInsightsConstructor:{version_string}\",\n",
    "    inputs=create_rai_inputs,\n",
    "    outputs=create_rai_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff017fa",
   "metadata": {},
   "source": [
    "Now, create an instance of each of our RAI tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e254f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the explanation\n",
    "explain_inputs = {\n",
    "   'comment': 'Insert text here',\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}'\n",
    "}\n",
    "explain_outputs = {\n",
    "    'explanation': None\n",
    "}\n",
    "explain_job = ComponentJob(\n",
    "    component=f\"RAIInsightsExplanation:{version_string}\",\n",
    "    inputs=explain_inputs,\n",
    "    outputs=explain_outputs\n",
    ")\n",
    "\n",
    "# Setup causal\n",
    "causal_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'treatment_features': '[\"Age\", \"Sex\"]',\n",
    "    'heterogeneity_features': '[\"Marital Status\"]'\n",
    "}\n",
    "causal_outputs = {\n",
    "    'causal': None\n",
    "}\n",
    "causal_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCausal:{version_string}\",\n",
    "    inputs=causal_inputs,\n",
    "    outputs=causal_outputs\n",
    ")\n",
    "\n",
    "# Setup counterfactual\n",
    "counterfactual_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'total_CFs': '10',\n",
    "    'desired_class': 'opposite'\n",
    "}\n",
    "counterfactual_outputs = {\n",
    "    'counterfactual': None\n",
    "}\n",
    "counterfactual_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCounterfactual:{version_string}\",\n",
    "    inputs=counterfactual_inputs,\n",
    "    outputs=counterfactual_outputs\n",
    ")\n",
    "\n",
    "# Setup error analysis\n",
    "error_analysis_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'filter_features': '[\"Race\", \"Sex\", \"Workclass\", \"Marital Status\", \"Country\", \"Occupation\"]'\n",
    "}\n",
    "error_analysis_outputs = {\n",
    "    'error_analysis': None\n",
    "}\n",
    "error_analysis_job = ComponentJob(\n",
    "    component=f\"RAIInsightsErrorAnalysis:{version_string}\",\n",
    "    inputs=error_analysis_inputs,\n",
    "    outputs=error_analysis_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d98393",
   "metadata": {},
   "source": [
    "Now the 'gather' component which assembles everything into an `RAIInsights` object, and computes the JSON for the UX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the gather component\n",
    "gather_inputs = {\n",
    "    'constructor': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'insight_1': '${{jobs.explain-job.outputs.explanation}}',\n",
    "    'insight_2': '${{jobs.causal-job.outputs.causal}}',\n",
    "    'insight_3': '${{jobs.counterfactual-job.outputs.counterfactual}}',\n",
    "    'insight_4': '${{jobs.error-analysis-job.outputs.error_analysis}}'\n",
    "}\n",
    "gather_outputs = {\n",
    "    'dashboard': None,\n",
    "    'ux_json': None\n",
    "}\n",
    "gather_job = ComponentJob(\n",
    "    component=f\"RAIInsightsGather:{version_string}\",\n",
    "    inputs=gather_inputs,\n",
    "    outputs=gather_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db19bf2",
   "metadata": {},
   "source": [
    "Finally, the pipeline itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4639a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Compute_Insights_from_Notebook_{version_string}\",\n",
    "    description=\"Python submitted Adult insights using fetched model\",\n",
    "    jobs={\n",
    "        'fetch-model-job': fetch_job,\n",
    "        'create-rai-job': create_rai_job,\n",
    "        'causal-job': causal_job,\n",
    "        'counterfactual-job': counterfactual_job,\n",
    "        'error-analysis-job': error_analysis_job,\n",
    "        'explain-job': explain_job,\n",
    "        'gather-job': gather_job\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=None,\n",
    "    compute=\"cpucluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fff73b",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a4915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute is not a known attribute of class <class 'azure.ml._restclient.v2021_10_01.models._models_py3.PipelineJob'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Finalizing\n",
      "Latest status : Completed\n"
     ]
    }
   ],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b63f92",
   "metadata": {},
   "source": [
    "# Download and display the insights\n",
    "Now we can download the insights we have computed. To start, we need to obtain the Run id of the 'gather-job' which ran as part of the previous pipeline. We have a helper for this, but the name of the experiment is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46b36de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (pywin32 302 (c:\\users\\ilmat\\miniconda3\\envs\\azureml_env\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n"
     ]
    }
   ],
   "source": [
    "from azure_ml_rai import list_rai_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b27324a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute_Insights_from_Notebook_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['09d86cde-ff83-4cc3-9d56-93a53423554a']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_list = list_rai_insights(ml_client, insights_pipeline_job.experiment_name)\n",
    "\n",
    "print(insights_pipeline_job.experiment_name)\n",
    "display(run_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add2092",
   "metadata": {},
   "source": [
    "We can use the mini SDK to download to a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai import download_rai_insights\n",
    "\n",
    "download_dir = 'my_downloaded_insight'\n",
    "\n",
    "download_rai_insights(\n",
    "    ml_client,\n",
    "    rai_insight_id=run_list[0],\n",
    "    path=download_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abe2d0",
   "metadata": {},
   "source": [
    "And with everything downloaded, we can load the RAIInsights object and instantiate the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0459d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai import RAIInsights\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "\n",
    "rai_i = RAIInsights.load(download_dir)\n",
    "\n",
    "ResponsibleAIDashboard(rai_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d3ffe",
   "metadata": {},
   "source": [
    "If for some reason we only need the JSON file holding the contents of `RAIInsights.get_data()`, we can download the other output port of the 'Gather' component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai import download_rai_insights_ux\n",
    "\n",
    "download_ux_dir = 'my_ux_insight'\n",
    "\n",
    "download_rai_insights_ux(\n",
    "    ml_client,\n",
    "    rai_insight_id=run_list[0],\n",
    "    path=download_ux_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eba47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

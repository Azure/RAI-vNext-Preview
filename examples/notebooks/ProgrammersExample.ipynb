{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121a2d97",
   "metadata": {},
   "source": [
    "First we fetch the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ad145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# programmers_train_data = pd.read_csv('C:\\\\fibberio\\\\sandbox\\\\programmers-train.csv', skipinitialspace=True)\n",
    "# programmers_test_data = pd.read_csv('C:\\\\fibberio\\\\sandbox\\\\programmers-test.csv', skipinitialspace=True)\n",
    "# programmers_train_data = programmers_train_data.drop(['first_name', 'last_name'], axis=1)\n",
    "# programmers_test_data = programmers_test_data.drop(['first_name', 'last_name'], axis=1)\n",
    "# del programmers_train_data[programmers_train_data.columns[0]]\n",
    "# del programmers_test_data[programmers_test_data.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmers_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# target_column_name = \"score\"\n",
    "\n",
    "# data_train = programmers_train_data\n",
    "# data_test = programmers_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcc05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = \"score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3192b",
   "metadata": {},
   "source": [
    "Now create an MLClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30a0f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\RAI-vNext-Preview\\config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d9ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train.to_parquet(\"programmers-train.parquet\")\n",
    "# data_test.to_parquet(\"programmers-test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c61496",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmers_version_string = '11'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b06c44",
   "metadata": {},
   "source": [
    "Upload the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd6b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'Programmers_Test_Data', 'description': 'RAI programmers test data', 'tags': {}, 'properties': {}, 'id': '/subscriptions/fac34303-435d-4486-8c3f-7094d82a0b60/resourceGroups/RAIPM/providers/Microsoft.MachineLearningServices/workspaces/RAIPM2/data/Programmers_Test_Data/versions/11', 'base_path': './', 'creation_context': <azure.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x00000181EA469250>, 'serialize': <msrest.serialization.Serializer object at 0x00000181EA45E910>, 'version': '11', 'latest_version': None, 'path': 'azureml://subscriptions/fac34303-435d-4486-8c3f-7094d82a0b60/resourcegroups/RAIPM/workspaces/RAIPM2/datastores/workspaceblobstore/paths/LocalUpload/ea1a713eaba89785ad8ecfb93cedb7f2/programmers-test.parquet', 'referenced_uris': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml.entities import Data\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "input_train_data = \"Programmers_Train_Data\"\n",
    "input_test_data = \"Programmers_Test_Data\"\n",
    "\n",
    "train_data = Data(\n",
    "    path=f\"programmers-train.parquet\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI programmers training data\",\n",
    "    name=input_train_data,\n",
    "    version=programmers_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(train_data)\n",
    "\n",
    "test_data = Data(\n",
    "    path=f\"programmers-test.parquet\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI programmers test data\",\n",
    "    name=input_test_data,\n",
    "    version=programmers_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f72fb",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "To simplify the model creation process, we're going to use a pipeline.\n",
    "\n",
    "Before we do anything else, we need to specify the version of the RAI components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '11'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55f3e6",
   "metadata": {},
   "source": [
    "Now we can create the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018ee1d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'component_src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31752/2711627417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'component_src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'component_src'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('component_src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd44c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting component_src/training_script_reg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile component_src/training_script_reg.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "def create_regression_pipeline(X, y):\n",
    "    pipe_cfg = {\n",
    "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
    "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
    "    }\n",
    "    num_pipe = Pipeline([\n",
    "        ('num_imputer', SimpleImputer(strategy='median')),\n",
    "        ('num_scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    feat_pipe = ColumnTransformer([\n",
    "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
    "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
    "    ])\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    pipeline = Pipeline(steps=[('preprocessor', feat_pipe),\n",
    "                               ('model', LinearRegression())])\n",
    "    return pipeline.fit(X, y)\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "    \n",
    "    #train_file = None\n",
    "    #for filename in os.listdir(args.training_data):\n",
    "    #    if filename.endswith('.csv'):\n",
    "    #        train_file = filename\n",
    "    #        break\n",
    "    #print(train_file)\n",
    "    #print(os.path.join(args.training_data, train_file))\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = create_regression_pipeline(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704878b",
   "metadata": {},
   "source": [
    "Now, we want to place this into a component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e38d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommandComponent({'auto_increment_version': False, 'is_anonymous': False, 'name': 'rai_programmers_training_component', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/fac34303-435d-4486-8c3f-7094d82a0b60/resourceGroups/RAIPM/providers/Microsoft.MachineLearningServices/workspaces/RAIPM2/components/rai_programmers_training_component/versions/11', 'base_path': './', 'creation_context': <azure.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x00000181EB5DA9D0>, 'serialize': <msrest.serialization.Serializer object at 0x00000181EB5DAD90>, 'command': 'python training_script_reg.py --training_data ${{inputs.training_data}} --target_column_name ${{inputs.target_column_name}} --model_output ${{outputs.model_output}}', 'code': '/subscriptions/fac34303-435d-4486-8c3f-7094d82a0b60/resourceGroups/RAIPM/providers/Microsoft.MachineLearningServices/workspaces/RAIPM2/codes/92dc7ea3-eb37-4588-b116-cbe50166ce64/versions/1', 'environment_variables': None, 'environment': '/subscriptions/fac34303-435d-4486-8c3f-7094d82a0b60/resourceGroups/RAIPM/providers/Microsoft.MachineLearningServices/workspaces/RAIPM2/environments/AML-RAI-Environment/versions/1', 'distribution': None, 'resources': {'instance_count': 1, 'properties': {}}, 'version': '11', 'latest_version': None, 'schema': 'http://azureml/sdk-2-0/CommandComponent.json', 'type': 'command', 'display_name': 'Programmers training component for RAI example', 'is_deterministic': True, 'inputs': {'training_data': {'name': 'training_data', 'optional': 'False', 'type': 'path'}, 'target_column_name': {'name': 'target_column_name', 'optional': 'False', 'type': 'string'}}, 'outputs': {'model_output': {'name': 'model_output', 'type': 'path'}}, 'yaml_str': None, 'other_parameter': {}, 'func': <function [component] Programmers training component for RAI example at 0x00000181EB585B80>})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml.entities import load_component\n",
    "\n",
    "yaml_contents = f\"\"\"\n",
    "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
    "name: rai_programmers_training_component\n",
    "display_name: Programmers training component for RAI example\n",
    "version: {programmers_version_string}\n",
    "type: command\n",
    "inputs:\n",
    "  training_data:\n",
    "    type: path\n",
    "  target_column_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: path\n",
    "code: ./component_src/\n",
    "environment: azureml:AML-RAI-Environment:1\n",
    "\"\"\" + r\"\"\"\n",
    "command: >-\n",
    "  python training_script_reg.py\n",
    "  --training_data ${{{{inputs.training_data}}}}\n",
    "  --target_column_name ${{{{inputs.target_column_name}}}}\n",
    "  --model_output ${{{{outputs.model_output}}}}\n",
    "\"\"\"\n",
    "\n",
    "yaml_filename = \"ProgrammersRegTrainingComp.yaml\"\n",
    "\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    f.write(yaml_contents.format(yaml_contents))\n",
    "    \n",
    "train_component_definition = load_component(\n",
    "    yaml_file=yaml_filename\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(train_component_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209c2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.ml.entities import Code, CommandComponent\n",
    "\n",
    "# training_code = Code(\n",
    "#     local_path='training_script_reg.py'\n",
    "# )\n",
    "\n",
    "# training_inputs = {\n",
    "#     'training_data': { 'type': 'path'},\n",
    "#     'target_column_name': { 'type': 'string'}\n",
    "# }\n",
    "\n",
    "# training_outputs = {\n",
    "#     'model_output': { 'type': 'path'}\n",
    "# }\n",
    "\n",
    "# training_component = CommandComponent(\n",
    "#     name=\"ProgrammersTestRegTrainingComponent\",\n",
    "#     version=\"5\",\n",
    "#     display_name=\"Simple reg training component\",\n",
    "#     code=training_code,\n",
    "#     environment=f\"AML-RAI-Environment:1\",\n",
    "#     inputs=training_inputs,\n",
    "#     outputs=training_outputs,\n",
    "#     command=\"python training_script_reg.py \" \\\n",
    "#             \"--training_data ${{inputs.training_data}} \" \\\n",
    "#             \"--target_column_name ${{inputs.target_column_name}} \" \\\n",
    "#             \"--model_output ${{outputs.model_output}}\"\n",
    "# )\n",
    "\n",
    "# ml_client.components.create_or_update(training_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cf52f",
   "metadata": {},
   "source": [
    "# Running a training pipeline\n",
    "Now we have a script which can train a model, we need to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afecca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute: cpucluster\n"
     ]
    }
   ],
   "source": [
    "compute_name = \"cpucluster\"\n",
    "\n",
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_DS2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341c61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'my_trained_reg_nb_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66e3e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl, Input\n",
    "\n",
    "register_component = load_component(\n",
    "    client=ml_client, name=\"register_model\", version=version_string\n",
    ")\n",
    "train_model_component = load_component(\n",
    "    client=ml_client, name=\"rai_programmers_training_component\", version=programmers_version_string\n",
    ")\n",
    "programmers_train_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_train_data}:{programmers_version_string}\", mode=\"download\"\n",
    ")\n",
    "programmers_test_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_test_data}:{programmers_version_string}\", mode=\"download\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_name,\n",
    "    description=\"Register Model for RAI Programmers example\",\n",
    "    experiment_name=f\"RAI_Programmers_Example_Model_Training_{model_name_suffix}\",\n",
    ")\n",
    "def my_training_pipeline(target_column_name, training_data):\n",
    "    trained_model = train_component_definition(\n",
    "        target_column_name=target_column_name,\n",
    "        training_data=programmers_train_pq\n",
    "    )\n",
    "\n",
    "    _ = register_component(\n",
    "        model_input_path=trained_model.outputs.model_output,\n",
    "        model_base_name=model_name,\n",
    "        model_name_suffix=model_name_suffix,\n",
    "    )\n",
    "\n",
    "    return {}\n",
    "\n",
    "model_registration_pipeline_job = my_training_pipeline(target_feature, programmers_train_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8af1dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Completed\n"
     ]
    }
   ],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job\n",
    "\n",
    "# This is the actual submission\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c37daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176ff640",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_model_component = load_component(\n",
    "    client=ml_client, name='fetch_registered_model', version=version_string\n",
    ")\n",
    "\n",
    "rai_constructor_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_constructor\", version=version_string\n",
    ")\n",
    "\n",
    "rai_explanation_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_explanation\", version=version_string\n",
    ")\n",
    "\n",
    "rai_causal_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_causal\", version=version_string\n",
    ")\n",
    "\n",
    "rai_counterfactual_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_counterfactual\", version=version_string\n",
    ")\n",
    "\n",
    "rai_erroranalysis_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_erroranalysis\", version=version_string\n",
    ")\n",
    "\n",
    "rai_gather_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_gather\", version=version_string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "683d2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@dsl.pipeline(\n",
    "        compute=compute_name,\n",
    "        description=\"Example RAI computation on programmers data\",\n",
    "        experiment_name=f\"RAI_Programmers_Example_RAIInsights_Computation_{model_name_suffix}\",\n",
    "    )\n",
    "def rai_classification_pipeline(\n",
    "        target_column_name,\n",
    "        train_data,\n",
    "        test_data,\n",
    "    ):\n",
    "        # Fetch the model\n",
    "        fetch_job = fetch_model_component(\n",
    "            model_id=expected_model_id\n",
    "        )\n",
    "        \n",
    "        # Initiate the RAIInsights\n",
    "        create_rai_job = rai_constructor_component(\n",
    "            title=\"RAI Dashboard Example\",\n",
    "            task_type=\"regression\",\n",
    "            model_info_path=fetch_job.outputs.model_info_output_path,\n",
    "            train_dataset=train_data,\n",
    "            test_dataset=test_data,\n",
    "            target_column_name=target_column_name,\n",
    "            categorical_column_names='[\"location\", \"style\", \"job title\", \"OS\", \"Employer\", \"IDE\", \"Programming language\"]'\n",
    "        )\n",
    "        \n",
    "        # Add an explanation\n",
    "        explain_job = rai_explanation_component(\n",
    "            comment=\"Explanation for the programmers dataset\",\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        \n",
    "        # Add causal analysis\n",
    "        causal_job = rai_causal_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            treatment_features='[\"Number of github repos contributed to\", \"YOE\"]',\n",
    "        )\n",
    "        \n",
    "        # Add counterfactual analysis\n",
    "        counterfactual_job = rai_counterfactual_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            total_cfs=10,  # Bug filed - should be total_CFs,\n",
    "            desired_range='[5, 10]'\n",
    "        )\n",
    "        \n",
    "        # Add error analysis\n",
    "        erroranalysis_job = rai_erroranalysis_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            filter_features='[\"style\", \"Employer\"]'\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        rai_gather_job = rai_gather_component(\n",
    "            constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            insight_1=explain_job.outputs.explanation,\n",
    "            insight_2=causal_job.outputs.causal,\n",
    "            insight_3=counterfactual_job.outputs.counterfactual,\n",
    "            insight_4=erroranalysis_job.outputs.error_analysis,\n",
    "        )\n",
    "\n",
    "        rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "        rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "        return {\n",
    "            \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "            \"ux_json\": rai_gather_job.outputs.ux_json,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29694946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_classification_pipeline(\n",
    "    target_column_name=target_feature,\n",
    "    train_data=programmers_train_pq,\n",
    "    test_data=programmers_test_pq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Completed\n"
     ]
    }
   ],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddcc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb002542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb52cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e6ff94",
   "metadata": {},
   "source": [
    "This is going to be a two component pipeline. The first will be the one we created above, which will train our model. The second will register it in AzureML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f93027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall inputs for the pipeline\n",
    "\n",
    "pipeline_inputs = {\n",
    "    'target_column_name': target_column_name,\n",
    "    'my_training_data': JobInput(dataset=f\"Programmers_Train_Data:\" + programmers_version_string, mode=\"download\"),\n",
    "    'my_test_data': JobInput(dataset=f\"Programmers_Test_Data:\" + programmers_version_string, mode=\"download\")\n",
    "}\n",
    "\n",
    "# Specify the training job\n",
    "train_job_inputs = {\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'training_data': '${{inputs.my_training_data}}',\n",
    "}\n",
    "train_job_outputs = {\n",
    "    'model_output': None\n",
    "}\n",
    "train_job = ComponentJob(\n",
    "    component=f\"ProgrammersTestRegTrainingComponent:5\",\n",
    "    inputs=train_job_inputs,\n",
    "    outputs=train_job_outputs\n",
    ")\n",
    "\n",
    "# The model registration job\n",
    "register_job_inputs = {\n",
    "    'model_input_path': '${{jobs.train-model-job.outputs.model_output}}',\n",
    "    'model_base_name': model_name,\n",
    "    'model_name_suffix': model_name_suffix\n",
    "}\n",
    "register_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "register_job = ComponentJob(\n",
    "    component=f\"RegisterModel:{version_string}\",\n",
    "    inputs=register_job_inputs,\n",
    "    outputs=register_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9970f",
   "metadata": {},
   "source": [
    "With our jobs specified, assemble them into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2410db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registration_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Register_Reg_Model_From_Notebook_01\",\n",
    "    description=\"Create and register a model from a notebook\",\n",
    "    jobs={\n",
    "        'train-model-job': train_job,\n",
    "        'register-model-job': register_job,\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=register_job_outputs,\n",
    "    compute=\"rai-cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a6422",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d450922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42543ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the actual submission\n",
    "\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f9e4",
   "metadata": {},
   "source": [
    "# Creating the RAI Insights\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered (this is not straightforward since the Register Model component is used in testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b893bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8c009",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are three 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty RAI dashboard\n",
    "1. Run the RAI tool components\n",
    "\n",
    "The job to fetch the registered model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be necessary once models are types within the pipeline graph\n",
    "\n",
    "fetch_job_inputs = {\n",
    "    'model_id': expected_model_id\n",
    "}\n",
    "fetch_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "fetch_job = ComponentJob(\n",
    "    component=f\"FetchRegisteredModel:{version_string}\",\n",
    "    inputs=fetch_job_inputs,\n",
    "    outputs=fetch_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b192f",
   "metadata": {},
   "source": [
    "With this registered model (and our datasets), we can create an empty RAI dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level RAI Insights component\n",
    "\n",
    "# We will reuse the same pipeline_inputs object in the end\n",
    "create_rai_inputs = {\n",
    "    'title': 'Run built from a Notebook',\n",
    "    'task_type': 'regression',\n",
    "    'model_info_path': '${{jobs.fetch-model-job.outputs.model_info_output_path}}',\n",
    "    'train_dataset': '${{inputs.my_training_data}}',\n",
    "    'test_dataset': '${{inputs.my_test_data}}',\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'categorical_column_names': '[\"location\", \"style\", \"job title\", \"OS\", \"Employer\", \"IDE\", \"Programming language\"]',\n",
    "}\n",
    "create_rai_outputs = {\n",
    "    'rai_insights_dashboard': None # Could theoretically redirect the datastore here\n",
    "}\n",
    "create_rai_job = ComponentJob(\n",
    "    component=f\"RAIInsightsConstructor:{version_string}\",\n",
    "    inputs=create_rai_inputs,\n",
    "    outputs=create_rai_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff017fa",
   "metadata": {},
   "source": [
    "Now, create an instance of each of our RAI tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the explanation\n",
    "explain_inputs = {\n",
    "   'comment': 'Insert text here',\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}'\n",
    "}\n",
    "explain_outputs = {\n",
    "    'explanation': None\n",
    "}\n",
    "explain_job = ComponentJob(\n",
    "    component=f\"RAIInsightsExplanation:{version_string}\",\n",
    "    inputs=explain_inputs,\n",
    "    outputs=explain_outputs\n",
    ")\n",
    "\n",
    "# Setup causal\n",
    "causal_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'treatment_features': '[\"Number of github repos contributed to\", \"YOE\"]'\n",
    "}\n",
    "causal_outputs = {\n",
    "    'causal': None\n",
    "}\n",
    "causal_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCausal:{version_string}\",\n",
    "    inputs=causal_inputs,\n",
    "    outputs=causal_outputs\n",
    ")\n",
    "\n",
    "# Setup counterfactual\n",
    "counterfactual_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'total_CFs': '10',\n",
    "    'desired_range': '[5, 10]'\n",
    "}\n",
    "counterfactual_outputs = {\n",
    "    'counterfactual': None\n",
    "}\n",
    "counterfactual_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCounterfactual:{version_string}\",\n",
    "    inputs=counterfactual_inputs,\n",
    "    outputs=counterfactual_outputs\n",
    ")\n",
    "\n",
    "# Setup error analysis\n",
    "error_analysis_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'filter_features': '[\"style\", \"Employer\"]'\n",
    "}\n",
    "error_analysis_outputs = {\n",
    "    'error_analysis': None\n",
    "}\n",
    "error_analysis_job = ComponentJob(\n",
    "    component=f\"RAIInsightsErrorAnalysis:{version_string}\",\n",
    "    inputs=error_analysis_inputs,\n",
    "    outputs=error_analysis_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d98393",
   "metadata": {},
   "source": [
    "Now the 'gather' component which assembles everything into an `RAIInsights` object, and computes the JSON for the UX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the gather component\n",
    "gather_inputs = {\n",
    "    'constructor': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'insight_1': '${{jobs.explain-job.outputs.explanation}}',\n",
    "    'insight_2': '${{jobs.causal-job.outputs.causal}}',\n",
    "    'insight_3': '${{jobs.counterfactual-job.outputs.counterfactual}}',\n",
    "    'insight_4': '${{jobs.error-analysis-job.outputs.error_analysis}}'\n",
    "}\n",
    "gather_outputs = {\n",
    "    'dashboard': None,\n",
    "    'ux_json': None\n",
    "}\n",
    "gather_job = ComponentJob(\n",
    "    component=f\"RAIInsightsGather:{version_string}\",\n",
    "    inputs=gather_inputs,\n",
    "    outputs=gather_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db19bf2",
   "metadata": {},
   "source": [
    "Finally, the pipeline itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4639a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Compute_Insights_from_Notebook_{version_string}\",\n",
    "    description=\"Python submitted Orange insights using fetched model\",\n",
    "    jobs={\n",
    "        'fetch-model-job': fetch_job,\n",
    "        'create-rai-job': create_rai_job,\n",
    "        'causal-job': causal_job,\n",
    "        'counterfactual-job': counterfactual_job,\n",
    "        'error-analysis-job': error_analysis_job,\n",
    "        'explain-job': explain_job,\n",
    "        'gather-job': gather_job\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=None,\n",
    "    compute=\"rai-cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fff73b",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b63f92",
   "metadata": {},
   "source": [
    "# Download and display the insights\n",
    "Now we can download the insights we have computed. To start, we need to obtain the Run id of the 'gather-job' which ran as part of the previous pipeline. We have a helper for this, but the name of the experiment is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b36de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai import list_rai_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27324a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = list_rai_insights(ml_client, insights_pipeline_job.experiment_name)\n",
    "\n",
    "print(insights_pipeline_job.experiment_name)\n",
    "display(run_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add2092",
   "metadata": {},
   "source": [
    "We can use the mini SDK to download to a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai import download_rai_insights\n",
    "\n",
    "download_dir = 'my_downloaded_insight'\n",
    "\n",
    "download_rai_insights(\n",
    "    ml_client,\n",
    "    rai_insight_id=run_list[0],\n",
    "    path=download_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abe2d0",
   "metadata": {},
   "source": [
    "And with everything downloaded, we can load the RAIInsights object and instantiate the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0459d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai import RAIInsights\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "\n",
    "rai_i = RAIInsights.load(download_dir)\n",
    "\n",
    "ResponsibleAIDashboard(rai_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d3ffe",
   "metadata": {},
   "source": [
    "If for some reason we only need the JSON file holding the contents of `RAIInsights.get_data()`, we can download the other output port of the 'Gather' component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai import download_rai_insights_ux\n",
    "\n",
    "download_ux_dir = 'my_ux_insight'\n",
    "\n",
    "download_rai_insights_ux(\n",
    "    ml_client,\n",
    "    rai_insight_id=run_list[0],\n",
    "    path=download_ux_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eba47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

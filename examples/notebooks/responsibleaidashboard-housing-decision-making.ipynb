{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7268a6b7",
   "metadata": {},
   "source": [
    "# Plan house improvements using causal analysis\n",
    "\n",
    "This notebook demonstrates the use of the Responsible AI Toolbox to make renovation decisions from historic apartments pricing data. It is based on a [similar notebook in the `responsible-ai-toolbox` repository](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-decision-making.ipynb), but updated to use the AzureML Responsible AI components.\n",
    "\n",
    "First, we need to specify the version of the RAI components which are available in the workspace. This was specified when the components were uploaded, and will have defaulted to '1':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '1652102651'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf947593",
   "metadata": {},
   "source": [
    "We also need to give the name of the compute cluster we want to use in AzureML. Later in this notebook, we will create it if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = \"cpucluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10207568",
   "metadata": {},
   "source": [
    "Finally, we need to specify a version for the data and components we will create while running this notebook. This should be unique for the workspace, but the specific value doesn't matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_house_improvement_version_string = '13'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f834b79",
   "metadata": {},
   "source": [
    "## Accessing the Data\n",
    "\n",
    "The following section examines the code necessary to create datasets and a model using components in AzureML.\n",
    "\n",
    "### Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "def clean_data(X, y, target_feature):\n",
    "    features = X.columns.values.tolist()\n",
    "    classes = y[target_feature].unique().tolist()\n",
    "    pipe_cfg = {\n",
    "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
    "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
    "    }\n",
    "    num_pipe = Pipeline([\n",
    "        ('num_imputer', SimpleImputer(strategy='median')),\n",
    "        ('num_scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    feat_pipe = ColumnTransformer([\n",
    "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
    "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
    "    ])\n",
    "    X = feat_pipe.fit_transform(X)\n",
    "    print(pipe_cfg['cat_cols'])\n",
    "    return X, feat_pipe, features, classes\n",
    "\n",
    "target_feature = 'SalePriceK'\n",
    "categorical_features = []\n",
    "\n",
    "outdirname = 'responsibleai.12.28.21'\n",
    "zipfilename = outdirname + '.zip'\n",
    "\n",
    "try:\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.request import urlretrieve\n",
    "zipfilename = outdirname + '.zip'\n",
    "urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, zipfilename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as unzip:\n",
    "    unzip.extractall('.')\n",
    "\n",
    "all_data = pd.read_csv('apartments-train.csv')\n",
    "all_data = all_data.drop(['Sold_HigherThan_Median','SalePrice'], axis=1)\n",
    "X, y = split_label(all_data, target_feature)\n",
    "X_train_original, X_test_original, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=7)\n",
    "\n",
    "X_train, feat_pipe, features, classes = clean_data(X_train_original, y_train, target_feature)\n",
    "y_train = y_train[target_feature].to_numpy()\n",
    "\n",
    "X_test = feat_pipe.transform(X_test_original)\n",
    "y_test = y_test[target_feature].to_numpy()\n",
    "\n",
    "train_data_df = X_train_original.copy()\n",
    "train_data_df[target_feature] = y_train\n",
    "\n",
    "test_data_df = X_test_original.copy()\n",
    "test_data_df[target_feature] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb322a5d",
   "metadata": {},
   "source": [
    "### Get the Data to AzureML\n",
    "\n",
    "With the data now split into 'train' and 'test' DataFrames, we save them out to files in preparation for upload into AzureML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving to files\")\n",
    "\n",
    "train_filename = \"housing_improvement_train.parquet\"\n",
    "test_filename = \"housing_improvement_test.parquet\"\n",
    "\n",
    "train_data_df.to_parquet(train_filename, index=False)\n",
    "test_data_df.to_parquet(test_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53416a2",
   "metadata": {},
   "source": [
    "We are going to create two Datasets in AzureML, one for the train and one for the test datasets. The first step is to create an `MLClient` to perform the upload. The method we use assumes that there is a `config.json` file (downloadable from the Azure or AzureML portals) present in the same directory as this notebook file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f99848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53557b87",
   "metadata": {},
   "source": [
    "We can then define the Datasets, and create them in AzureML. This will also upload the Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80998ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "input_train_data = \"housing_improvement_train_pq\"\n",
    "input_test_data = \"housing_improvement_test_pq\"\n",
    "\n",
    "train_data = Data(\n",
    "    path=train_filename,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI housing improvement example training data\",\n",
    "    name=input_train_data,\n",
    "    version=rai_house_improvement_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(train_data)\n",
    "\n",
    "test_data = Data(\n",
    "    path=test_filename,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI housing improvement example test data\",\n",
    "    name=input_test_data,\n",
    "    version=rai_house_improvement_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e0a41",
   "metadata": {},
   "source": [
    "## A model training pipeline\n",
    "\n",
    "Firstly, we should note that for this example, we do not actually require a model; we are going to undertake a causal analysis which purely works on the data. However, for our Responsible AI components, a model is required, since the portal shows the analyses under the Model view. To simplify the model creation process, we're going to use a pipeline. This will have two stages:\n",
    "\n",
    "1. The actual training component\n",
    "1. A model registration component\n",
    "\n",
    "We have to register the model in AzureML in order for our RAI insights components to use it.\n",
    "\n",
    "### The Training Component\n",
    "\n",
    "The training component is for this particular model. In this case, we are going to use the `DummyRegressor` from SciKit-Learn.\n",
    "\n",
    "We start by creating a directory to hold the component source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('component_src', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ab65f",
   "metadata": {},
   "source": [
    "Now, the component source code itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54897d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile component_src/dummy_regressor_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = DummyRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae5efc",
   "metadata": {},
   "source": [
    "Now that the training script is saved on our local drive, we create a YAML file to describe it as a component to AzureML. This involves defining the inputs and outputs, specifing the AzureML environment which can run the script, and telling AzureML how to invoke the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import load_component\n",
    "\n",
    "yaml_contents = f\"\"\"\n",
    "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
    "name: rai_dummy_training_component\n",
    "display_name: Dummy Regressor component for RAI example\n",
    "version: {rai_house_improvement_version_string}\n",
    "type: command\n",
    "inputs:\n",
    "  training_data:\n",
    "    type: path\n",
    "  target_column_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: path\n",
    "code: ./component_src/\n",
    "environment: azureml:AML-RAI-Environment:{version_string}\n",
    "\"\"\" + r\"\"\"\n",
    "command: >-\n",
    "  python dummy_regressor_script.py\n",
    "  --training_data ${{{{inputs.training_data}}}}\n",
    "  --target_column_name ${{{{inputs.target_column_name}}}}\n",
    "  --model_output ${{{{outputs.model_output}}}}\n",
    "\"\"\"\n",
    "\n",
    "yaml_filename = \"RAIDummyTrainingComponent.yaml\"\n",
    "\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    f.write(yaml_contents.format(yaml_contents))\n",
    "    \n",
    "train_component_definition = load_component(\n",
    "    yaml_file=yaml_filename\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(train_component_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2b5a3",
   "metadata": {},
   "source": [
    "### Running a training pipeline\n",
    "\n",
    "The component to register the model is part of the suite of RAI components, so we do not have to define it here. As such, we are now ready to run the training pipeline itself.\n",
    "\n",
    "First, we need to make sure we have a compute cluster available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbea6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_DS2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47109e6",
   "metadata": {},
   "source": [
    "Next, we define the name under which we want to register the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6617a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'rai_housing_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c20e73",
   "metadata": {},
   "source": [
    "Next, we define the pipeline using objects from the AzureML SDKv2. As mentioned above, there are two component jobs: one to train the model, and one to register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e23592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl, Input\n",
    "\n",
    "register_component = load_component(\n",
    "    client=ml_client, name=\"register_model\", version=version_string\n",
    ")\n",
    "train_model_component = load_component(\n",
    "    client=ml_client, name=\"rai_dummy_training_component\", version=rai_house_improvement_version_string\n",
    ")\n",
    "housing_improvement_train_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_train_data}:{rai_house_improvement_version_string}\", mode=\"download\"\n",
    ")\n",
    "housing_improvement_test_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_test_data}:{rai_house_improvement_version_string}\", mode=\"download\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_name,\n",
    "    description=\"Register Model for RAI Housing Improvement example\",\n",
    "    experiment_name=f\"RAI_Housing_Improvement_Example_Model_Training_{model_name_suffix}\",\n",
    ")\n",
    "def my_training_pipeline(target_column_name, training_data):\n",
    "    trained_model = train_component_definition(\n",
    "        target_column_name=target_column_name,\n",
    "        training_data=training_data\n",
    "    )\n",
    "\n",
    "    _ = register_component(\n",
    "        model_input_path=trained_model.outputs.model_output,\n",
    "        model_base_name=model_name,\n",
    "        model_name_suffix=model_name_suffix,\n",
    "    )\n",
    "\n",
    "    return {}\n",
    "\n",
    "model_registration_pipeline_job = my_training_pipeline(target_feature, housing_improvement_train_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc9619",
   "metadata": {},
   "source": [
    "With the pipeline definition created, we can submit it to AzureML. We define a helper function to do the submission, which waits for the submitted job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8de990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job\n",
    "\n",
    "# This is the actual submission\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d7e07",
   "metadata": {},
   "source": [
    "##  Creating the RAI Insights\n",
    "\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26c810",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are four 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty `RAIInsights` object\n",
    "1. Run the RAI tool components\n",
    "1. Gather the tool outputs into a single `RAIInsights` object\n",
    "\n",
    "We start by loading the RAI component definitions for use in our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67852887",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_model_component = load_component(\n",
    "    client=ml_client, name='fetch_registered_model', version=version_string\n",
    ")\n",
    "\n",
    "rai_constructor_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_constructor\", version=version_string\n",
    ")\n",
    "\n",
    "rai_causal_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_causal\", version=version_string\n",
    ")\n",
    "\n",
    "rai_gather_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_gather\", version=version_string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37609069",
   "metadata": {},
   "source": [
    "Now the pipeline itself. This fetches the registered model, creates an empty `RAIInsights` object, adds the analyses, and then gathers everything into the final `RAIInsights` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@dsl.pipeline(\n",
    "        compute=compute_name,\n",
    "        description=\"Example RAI computation on housing improvement data\",\n",
    "        experiment_name=f\"RAI_Housing_Improvements_Example_RAIInsights_Computation_{model_name_suffix}\",\n",
    "    )\n",
    "def rai_classification_pipeline(\n",
    "        target_column_name,\n",
    "        train_data,\n",
    "        test_data,\n",
    "    ):\n",
    "        # Fetch the model\n",
    "        fetch_job = fetch_model_component(\n",
    "            model_id=expected_model_id\n",
    "        )\n",
    "        \n",
    "        # Initiate the RAIInsights\n",
    "        create_rai_job = rai_constructor_component(\n",
    "            title=\"RAI Dashboard Example\",\n",
    "            task_type=\"classification\",\n",
    "            model_info_path=fetch_job.outputs.model_info_output_path,\n",
    "            train_dataset=train_data,\n",
    "            test_dataset=test_data,\n",
    "            target_column_name=target_column_name,\n",
    "            categorical_column_names=json.dumps(categorical_features),\n",
    "        )\n",
    "        \n",
    "        # Add causal analysis\n",
    "        causal_job = rai_causal_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            treatment_features='[\"OverallCond\", \"OverallQual\", \"Fireplaces\", \"GarageCars\", \"ScreenPorch\"]',\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        rai_gather_job = rai_gather_component(\n",
    "            constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            insight_1=causal_job.outputs.causal,\n",
    "        )\n",
    "\n",
    "        rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "        rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "        return {\n",
    "            \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "            \"ux_json\": rai_gather_job.outputs.ux_json,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9104a",
   "metadata": {},
   "source": [
    "With all of our jobs defined, we can assemble them into the pipeline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2617989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ml import Output\n",
    "\n",
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_classification_pipeline(\n",
    "    target_column_name=target_feature,\n",
    "    train_data=housing_improvement_train_pq,\n",
    "    test_data=housing_improvement_test_pq,\n",
    ")\n",
    "\n",
    "# Workaround to enable the download\n",
    "rand_path = str(uuid.uuid4())\n",
    "insights_pipeline_job.outputs.dashboard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.ux_json = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/ux_json/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d42662",
   "metadata": {},
   "source": [
    "Now, submit the pipeline job and wait for it to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83529dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea452238",
   "metadata": {},
   "source": [
    "The dashboard should appear on the Model Details page of the AzureML portal. The following cell computes the appropriate link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = ml_client._operation_scope.subscription_id\n",
    "rg_name = ml_client._operation_scope.resource_group_name\n",
    "ws_name = ml_client.workspace_name\n",
    "\n",
    "expected_uri = f\"https://ml.azure.com/model/{expected_model_id}/model_analysis?wsid=/subscriptions/{sub_id}/resourcegroups/{rg_name}/workspaces/{ws_name}\"\n",
    "\n",
    "print(f\"Please visit {expected_uri} to see your analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e25d7",
   "metadata": {},
   "source": [
    "Following the link should bring you to a page which looks similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260f971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d0199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df[target_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b55a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3245c4ac",
   "metadata": {},
   "source": [
    "# Assess disease progression predictions on diabetes data\n",
    "\n",
    "This is an adaptation of the [corresponding notebook in the `responsible-ai-toolbox` repository](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/responsibleaidashboard-diabetes-regression-model-debugging.ipynb) to work with the Responsible AI components in AzureML.\n",
    "\n",
    "We will use the Responsible AI components to assess a regression trained on diabetes progression data. Next, we will walk through the API calls necessary to create a widget with model analysis insights, then undertake a visual analysis of the model.\n",
    "\n",
    "First, we need to specify the version of the RAI components which are available in the workspace. This was specified when the components were uploaded, and will have defaulted to '1':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac0832",
   "metadata": {},
   "source": [
    "We also need to give the name of the compute cluster we want to use in AzureML. Later in this notebook, we will create it if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ed890",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = \"cpucluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402748c",
   "metadata": {},
   "source": [
    "Finally, we need to specify a version for the data and components we will create while running this notebook. This should be unique for the workspace, but the specific value doesn't matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_diabetes_regression_example_version_string = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce352c01",
   "metadata": {},
   "source": [
    "## Accessing the data\n",
    "\n",
    "First, we need to obtain the dataset and upload it to our AzureML workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sklearn.datasets.load_diabetes()\n",
    "target_feature = 'y'\n",
    "continuous_features = data.feature_names\n",
    "data_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "data_df[target_feature] = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe46c8",
   "metadata": {},
   "source": [
    "Now, split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b27778",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data_df, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f875041",
   "metadata": {},
   "source": [
    "Write to parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"diabetes_regression_train.parquet\"\n",
    "test_filename = \"diabetes_regression_test.parquet\"\n",
    "\n",
    "data_train.to_parquet(train_filename, index=False)\n",
    "data_test.to_parquet(test_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bca5a",
   "metadata": {},
   "source": [
    "We are going to create two Datasets in AzureML, one for the train and one for the test datasets. The first step is to create an `MLClient` to perform the upload. The method we use assumes that there is a `config.json` file (downloadable from the Azure or AzureML portals) present in the same directory as this notebook file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0eb11",
   "metadata": {},
   "source": [
    "We can then define and upload the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "input_train_data = \"adult_train_pq\"\n",
    "input_test_data = \"adult_test_pq\"\n",
    "\n",
    "train_data = Data(\n",
    "    path=train_filename,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI diabetes regression example training data\",\n",
    "    name=input_train_data,\n",
    "    version=rai_diabetes_regression_example_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(train_data)\n",
    "\n",
    "test_data = Data(\n",
    "    path=train_filename,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI diabetes regression example test data\",\n",
    "    name=input_test_data,\n",
    "    version=rai_diabetes_regression_example_version_string,\n",
    ")\n",
    "ml_client.data.create_or_update(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6e47c",
   "metadata": {},
   "source": [
    "## A model training pipeline\n",
    "\n",
    "To simplify the model creation process, we're going to use a pipeline. This will have two stages:\n",
    "\n",
    "1. The actual training component\n",
    "1. A model registration component\n",
    "\n",
    "We have to register the model in AzureML in order for our RAI insights components to use it.\n",
    "\n",
    "### The Training Component\n",
    "\n",
    "The training component is for this particular model. In this case, we are going to train a `RandomForestRegressor` on the input data and save it using MLFlow. We need command line arguments to specify the location of the input data, the location where MLFlow should write the output model, and the name of the target column in the dataset.\n",
    "\n",
    "We start by creating a directory to hold the component source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('component_src', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d796f04",
   "metadata": {},
   "source": [
    "Next, put our training script into the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile component_src/diabetes_regression_training_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a16a5",
   "metadata": {},
   "source": [
    "Now that the training script is saved on our local drive, we create a YAML file to describe it as a component to AzureML. This involves defining the inputs and outputs, specifing the AzureML environment which can run the script, and telling AzureML how to invoke the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import load_component\n",
    "\n",
    "yaml_contents = f\"\"\"\n",
    "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
    "name: rai_diabetes_regression_training_component\n",
    "display_name: Diabetes regression training component for RAI example\n",
    "version: {rai_diabetes_regression_example_version_string}\n",
    "type: command\n",
    "inputs:\n",
    "  training_data:\n",
    "    type: path\n",
    "  target_column_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: path\n",
    "code: ./component_src/\n",
    "environment: azureml:AML-RAI-Environment:{version_string}\n",
    "\"\"\" + r\"\"\"\n",
    "command: >-\n",
    "  python diabetes_regression_training_script.py\n",
    "  --training_data ${{{{inputs.training_data}}}}\n",
    "  --target_column_name ${{{{inputs.target_column_name}}}}\n",
    "  --model_output ${{{{outputs.model_output}}}}\n",
    "\"\"\"\n",
    "\n",
    "yaml_filename = \"RAIHousingTrainingComponent.yaml\"\n",
    "\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    f.write(yaml_contents.format(yaml_contents))\n",
    "    \n",
    "train_component_definition = load_component(\n",
    "    yaml_file=yaml_filename\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(train_component_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc2cdc",
   "metadata": {},
   "source": [
    "### Running a training pipeline\n",
    "\n",
    "The component to register the model is part of the suite of RAI components, so we do not have to define it here. As such, we are now ready to run the training pipeline itself.\n",
    "\n",
    "We start by ensuring that the compute cluster named above exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_DS2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab3db5",
   "metadata": {},
   "source": [
    "We continue by defining the name under which we want to register the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'rai_diabetes_regression_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f1ba7",
   "metadata": {},
   "source": [
    "Next, we define the pipeline using objects from the AzureML SDKv2. As mentioned above, there are two component jobs: one to train the model, and one to register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl, Input\n",
    "\n",
    "register_component = load_component(\n",
    "    client=ml_client, name=\"register_model\", version=version_string\n",
    ")\n",
    "train_model_component = load_component(\n",
    "    client=ml_client, name=\"rai_diabetes_regression_training_component\", version=rai_diabetes_regression_example_version_string\n",
    ")\n",
    "diabetes_train_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_train_data}:{rai_diabetes_regression_example_version_string}\", mode=\"download\"\n",
    ")\n",
    "diabetes_test_pq = Input(\n",
    "    type=\"uri_file\", path=f\"{input_test_data}:{rai_diabetes_regression_example_version_string}\", mode=\"download\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_name,\n",
    "    description=\"Register Model for RAI Diabetes Regression example\",\n",
    "    experiment_name=f\"RAI_Diabetes_Regression_Example_Model_Training_{model_name_suffix}\",\n",
    ")\n",
    "def my_training_pipeline(target_column_name, training_data):\n",
    "    trained_model = train_component_definition(\n",
    "        target_column_name=target_column_name,\n",
    "        training_data=training_data\n",
    "    )\n",
    "    trained_model.set_limits(timeout=120)\n",
    "\n",
    "    _ = register_component(\n",
    "        model_input_path=trained_model.outputs.model_output,\n",
    "        model_base_name=model_name,\n",
    "        model_name_suffix=model_name_suffix,\n",
    "    )\n",
    "\n",
    "    return {}\n",
    "\n",
    "model_registration_pipeline_job = my_training_pipeline(target_feature, diabetes_train_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc72ccf",
   "metadata": {},
   "source": [
    "With the pipeline definition created, we can submit it to AzureML. We define a helper function to do the submission, which waits for the submitted job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job\n",
    "\n",
    "# This is the actual submission\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9871d1",
   "metadata": {},
   "source": [
    "##  Creating the RAI Insights\n",
    "\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47443625",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are four 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty `RAIInsights` object\n",
    "1. Run the RAI tool components\n",
    "1. Gather the tool outputs into a single `RAIInsights` object\n",
    "\n",
    "We start by loading the RAI component definitions for use in our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_model_component = load_component(\n",
    "    client=ml_client, name='fetch_registered_model', version=version_string\n",
    ")\n",
    "\n",
    "rai_constructor_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_constructor\", version=version_string\n",
    ")\n",
    "\n",
    "rai_counterfactual_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_counterfactual\", version=version_string\n",
    ")\n",
    "\n",
    "rai_explanation_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_explanation\", version=version_string\n",
    ")\n",
    "\n",
    "rai_erroranalysis_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_erroranalysis\", version=version_string\n",
    ")\n",
    "\n",
    "rai_gather_component = load_component(\n",
    "    client=ml_client, name=\"rai_insights_gather\", version=version_string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07405202",
   "metadata": {},
   "source": [
    "Now the pipeline itself. This fetches the registered model, creates an empty `RAIInsights` object, adds the analyses, and then gathers everything into the final `RAIInsights` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@dsl.pipeline(\n",
    "        compute=compute_name,\n",
    "        description=\"Example RAI computation on diabetes regression data\",\n",
    "        experiment_name=f\"RAI_Diabetes_Regression_Example_RAIInsights_Computation_{model_name_suffix}\",\n",
    "    )\n",
    "def rai_classification_pipeline(\n",
    "        target_column_name,\n",
    "        train_data,\n",
    "        test_data,\n",
    "    ):\n",
    "        # Fetch the model\n",
    "        fetch_job = fetch_model_component(\n",
    "            model_id=expected_model_id\n",
    "        )\n",
    "        \n",
    "        # Initiate the RAIInsights\n",
    "        create_rai_job = rai_constructor_component(\n",
    "            title=\"RAI Dashboard Example\",\n",
    "            task_type=\"regression\",\n",
    "            model_info_path=fetch_job.outputs.model_info_output_path,\n",
    "            train_dataset=train_data,\n",
    "            test_dataset=test_data,\n",
    "            target_column_name=target_column_name,\n",
    "        )\n",
    "        \n",
    "        # Add an explanation\n",
    "        explain_job = rai_explanation_component(\n",
    "            comment=\"Explanation for the diabetes regression dataset\",\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        \n",
    "        # Add error analysis\n",
    "        erroranalysis_job = rai_erroranalysis_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        \n",
    "        # Add counterfactual analysis\n",
    "        counterfactual_job = rai_counterfactual_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            total_cfs=20,\n",
    "            desired_range='[50, 120]',\n",
    "        )\n",
    "        counterfactual_job.set_limits(timeout=600)\n",
    "\n",
    "        # Combine everything\n",
    "        rai_gather_job = rai_gather_component(\n",
    "            constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            insight_1=explain_job.outputs.explanation,\n",
    "            insight_3=counterfactual_job.outputs.counterfactual,\n",
    "            insight_4=erroranalysis_job.outputs.error_analysis,\n",
    "        )\n",
    "\n",
    "        rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "        rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "        return {\n",
    "            \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "            \"ux_json\": rai_gather_job.outputs.ux_json,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb9fd2",
   "metadata": {},
   "source": [
    "Next, we define the pipeline object itself, and ensure that the outputs will be available for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92370f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ml import Output\n",
    "\n",
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_classification_pipeline(\n",
    "    target_column_name=target_feature,\n",
    "    train_data=diabetes_train_pq,\n",
    "    test_data=diabetes_test_pq,\n",
    ")\n",
    "\n",
    "# Workaround to enable the download\n",
    "rand_path = str(uuid.uuid4())\n",
    "insights_pipeline_job.outputs.dashboard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.ux_json = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/ux_json/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf817fa7",
   "metadata": {},
   "source": [
    "And submit the pipeline to AzureML for execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689f7f4",
   "metadata": {},
   "source": [
    "The dashboard should appear in the AzureML portal in the registered model view. The following cell computes the expected URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = ml_client._operation_scope.subscription_id\n",
    "rg_name = ml_client._operation_scope.resource_group_name\n",
    "ws_name = ml_client.workspace_name\n",
    "\n",
    "expected_uri = f\"https://ml.azure.com/model/{expected_model_id}/model_analysis?wsid=/subscriptions/{sub_id}/resourcegroups/{rg_name}/workspaces/{ws_name}\"\n",
    "\n",
    "print(f\"Please visit {expected_uri} to see your analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6a310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
